---
title: "Exercises E"
author: "Gerko Vink"
date: "Data Science and Predictive Machine Learning"
output: 
   html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
---
  
<style type="text/css">
  
body{ /* Normal  */
  font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
  font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
  font-size: 14px;
}
</style>

We use the following packages:
```{r warning=FALSE, message=FALSE}
library(mice)
library(dplyr)
library(magrittr)
library(DAAG)
library(caret)
```

---


The following table shows numbers of occasions when inhibition (i.e., no flow of current across a membrane) occurred within 120 s, for different concentrations of the protein peptide-C. The outcome `yes` implies that inhibition has occurred.

    conc 0.1 0.5  1 10 20 30 50 70 80 100 150 
    no     7   1 10  9  2  9 13  1  1   4   3 
    yes    0   0  3  4  0  6  7  0  0   1   7

---

# Exercises

---

1. **Create this data in `R`. Make it into a data frame.**
```{r}
data <- data.frame(conc = c(.1, .5, 1, 10, 20, 30, 50, 70, 80, 100, 150),
                   no = c(7, 1, 10, 9, 2, 9, 13, 1, 1, 4, 3),
                   yes = c(0, 0, 3, 4, 0, 6, 7, 0, 0, 1 ,7)) 
data
```


---

2. **Add the following three new variables (columns) to the data**

- the margin of `no` and `yes` over the rows (i.e. `no` + `yes`)
- the proportion (`yes` over `margin`)
- the logodds

First, we create a function to calculate the logit (logodds):
```{r}
logit <- function(p) log(p / (1 - p))
```

Then we add the new columns
```{r}
data <- 
  data %>% 
  mutate(margin = no+yes,
         prop = yes / margin,
         logit = logit(prop)) # apply the function
```

---

3. **Inspect the newly added columns in thw data set. What do you see?**
```{r}
data
```

There are a lot of zero proportions, hence the $-\infty$ in the logit. You can fix this (at least the interpretation of the `logodds`) by adding a constant (usually 0.5) to all cells conform the empirical `logodds` [(see e.g. Cox and Snell 1989)](http://www.amazon.com/Analysis-Edition-Monographs-Statistics-Probability/dp/0412306204). 

Another option is to add a value of `1`. This is conceptually interesting as the log of `1` equals `r log(1)`. 

---

4. **Add a new column where the log odds are calculated as:**
$$\log(\text{odds}) = \log\left(\frac{\text{yes} + 0.5}{\text{no} + 0.5}\right)$$
```{r}
logitCandS <- function(yes, no) log((yes + .5) / (no + .5))
data <- 
  data %>% 
  mutate(logitCS = logitCandS(yes, no))
data
```
We can now see that the $-\infty$ proportions are gone.

---

5. **Fit the model with `margin` as the weights, just like the model in slide 44 from this week's lecture**
```{r}
fit <- 
  data %$%
  glm(prop ~ conc, family=binomial(link = "logit"), weights=margin)
```

---

6. **Look at the summary of the fitted object**
```{r}
summary(fit)
```

A unit increase in `conc` increases the $\log(\text{odds})$ of `prop` with `0.01215`. This increase is significant. 

---

Just like with a linear model, we can obtain a series of plots for inspecting the logistic model. 

---

7. **Inspect the plots number 1 and 5 for object `fit`**
```{r}
plot(fit, which = c(1, 5))
```

The data set is small, but case `11` stands out in the `Residuals vs. Leverage` plot. Case 11 has quite a lot of leverage, although its residual is not out of the ordinary. Its leverage, however is sufficient to yield it a Cook's distance of over 1. Further investigation is warranted. 

---

Many models are built around the assumption of normality. Even in cases when this assumption is not strict, modeling efforts may benefit from a transformation towards normality. A transformation that is often used for skewed data is the log-transformation.

---

8. **`conc` is somewhat skewed. Plot the density of the `conc` column twice:**

- once as observed
- once with a log-tranformation

```{r echo = FALSE}
par(mfrow = c(1, 2)) # change parameter of plotting to 2 plots side-by-side
data$conc %>%
  density() %>%
  plot(main = "density", xlab = "conc")
data$conc %>% 
  log() %>%
  density() %>%
  plot(main = "density", xlab = "log(conc)")
```

In this case the transformation towards normality is not very apparent due to the small sample size. If we take, e.g. from the `mice::mammalsleep` data set the column `bw` (body weigth), the log-transformation is very beneficial:

```{r}
par(mfrow = c(1, 2)) # change parameter of plotting to 2 plots side-by-side
mammalsleep %$%
  density(bw) %>%
  plot(main = "density", xlab = "body weight")

mammalsleep %$% 
  log(bw) %>%
  density() %>%
  plot(main = "density", xlab = "log(body weight)")
```

I hope you all see that taking the log of `bw` in this case would make the predictor far more normal.

---

We now return to the exercise and its data example with `prop` and `conc`.

---

9. **Investigate the `log` model**

To apply this transformation in the model directly, we best pose it in the `I()` function. `I()` indicates that any interpretation and/or conversion of objects should be inhibited and the contents should be evaluated 'as is'. For example, to run the model:

```{r}
fit.log <- 
  data %$%
  glm(prop ~ I(log(conc)), family=binomial, weights=margin)
```

---

9. **Look at the summary of the fitted objects again**
```{r}
summary(fit.log)
```
The logodds for `fit.log` now depict the unit increase in `log(conc)`, instead of `conc`.

---

10. **Inspects the plots number 1 and 5 of the fitted objects based on `log(conc)`**
```{r}
plot(fit.log, which = c(1, 5))
```

Outliers are now less of an issue. This exercise demonstrates that data transformations may easily render our method more valid, but in exchange it makes our model interpretation more difficult: **Parameters now have to be assessed in the `log(conc)` parameter space**. 

---

11. **Use the `brandsma` data from package `mice` to fit a logistic regression model for `sex` based on `lpo` (Language Post Outcome).**
```{r}
brandsma.subset <- 
  brandsma %>%
  subset(!is.na(sex) & !is.na(lpo), select = c(sex, lpo))

fit <- 
  brandsma.subset %$%
  glm(sex ~ lpo, family=binomial(link='logit'))

fit %>%
  summary()
```

With every unit increase in `lpo`, the logodds of gender increases by `r coef(fit)[2]`. 

---

12. **Obtain confidence intervals for the parameter estimates.**
```{r}
confint(fit)
```

---

13. **Use the model parameters to predict the `sex` variable and compare your predictions to the observed `sex`. **

We can obtain predictions by using function `predict`. The default predictions are on the scale of the linear predictors; the alternative "response" is on the scale of the response variable. Thus for a default binomial model the default predictions are of log-odds (probabilities on logit scale) and type = "response" gives the predicted probabilities. 

To obtain the predicted logodds:
```{r}
pred.logodds <- 
  fit %>%
  predict()
head(pred.logodds)
```
and the predicted probabilities
```{r}
pred.prob <- 
  fit %>%
  predict(type = "response")
head(pred.prob)
```

We can then use the decision boundary `pred.prob > .5` to assign cases to `sex == 1` and the others to `sex == 0`. 
```{r}
pred <- factor(ifelse(pred.prob > .5, 1, 0))
```

To determine how many correct predictions we have, we can use
```{r}
obs <- 
  brandsma %>%
  filter(!is.na(sex) & !is.na(lpo)) # in order to obtain the same rows

confusionMatrix(pred, factor(obs$sex))

```
So we succesfully predict a little over half the values. That is not so good, because, based on chance alone, we would expect to successfully predict about half:

---

An quick way to perform crossvalidation is with `DAAG::CVbinary()`:

```{r}
CVbinary(glm(sex ~ lpo, family=binomial(link='logit'), data = brandsma.subset))
```

---

The lesson here is that a significant parameter has no meaning if the substantive interpretation of the effect is ignored. There is almost no relation, whatsoever, there is just sufficient data to deem the influence of `lpo` on `sex` worthy of significance. 

---

14. **In the data set `minor.head.injury` (from package `DAAG`), obtain a logistic regression model relating `clinically.important.brain.injury` to all the other variables.**

Let us fit the model, predict `clinically.important.brain.injury` by all other variables in the data.
```{r}
fit <- glm(clinically.important.brain.injury ~ ., family=binomial, data=head.injury) 
summary(fit)
```


---

15. **Patients whose risk is sufficiently high will be sent for CT (computed tomography). Using a risk threshold of 0.025 (2.5%), turn the result into a decision rule for use of CT.**

A risk of 2.5% corresponds to the cutoff for a CT scan. This translates to a logit of $\log\left(\frac{.025}{1-.025}\right) = -3.663562$. In other words, any sum of variables that "lifts" the intercept above -3.66 would satisfy the cutoff. 

---

End of `Practical`. 