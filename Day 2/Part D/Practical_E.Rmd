---
title: "Exercises D"
author: "Gerko Vink"
date: "Data Science and Predictive Machine Learning"
output: 
   html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
---
  
<style type="text/css">
  
body{ /* Normal  */
  font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
  font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
  font-size: 14px;
}
</style>

We use the following packages:
```{r warning=FALSE, message=FALSE}
library(mice)
library(caret)
library(dplyr)
library(magrittr)
library(DAAG)
library(readr)
```

---

We use the `titanic` data set for this exercise. Download the [`titanic.csv`](titanic.csv) data set. 

```{r}
con <- url("https://www.gerkovink.com/erasmus/Day%202/Part%20D/titanic.csv")
titanic <- read_csv(con)
```

---

__Exercise 1__ **Inspect the titanic data set by calling `titanic` in the console and with functions `summary()`, `str()` and `md.pattern()`.**

```{r}
titanic
```
We can see that the `titanic` data set is imported as a `tibble`. A `tibble` is a more flexible data frame with a much nicer printing class.
```{r}
summary(titanic)
```
The `summary()` output gives us direct information about the parametric nature of the columns is the data
```{r}
str(titanic)
```
When we study the structure of the data set, we see that the outcome `Survived` is not coded as a `factor`, but as a numeric column. The same holds for `Pclass`. This will influence the default estimation later on. There are more irregularities, but we'll ignore those for now.
```{r}
md.pattern(titanic, rotate.names = TRUE)
```
There are no missing values in this `titanic` data set. 

---

__Exercise 2__ **Correct the measurement level of the columns `Pclass` and `Survived`. Then ask for the `summary()` once more.**
```{r}
titanic %<>% 
  mutate(Pclass = factor(Pclass, labels = c("1st class", "2nd class", "3rd class")), 
         Survived = factor(Survived, labels = c("No", "Yes")))
titanic %>% summary()
```
We now see the tabular information about the `Survived` and `Pclass` columns. This is because these columns are now coded as factors (i.e. categorical variables with a numeric representation). Note that in the `mutate` call, I used the ` %<>% ` pipe. This *assign* pipe returns the endresult of the pipe to the original object. This mitigates the use of the `<-` assign operator and the double calling of the `titanic` set in the regular strategy below:

```{r eval = FALSE}
titanic <- titanic %>% 
  mutate(Pclass = factor(Pclass, labels = c("1st class", "2nd class", "3rd class")), 
         Survived = factor(Survived, labels = c("No", "Yes")))
```


---

# Data subsetting

---

__Exercise 3__ **Split the data manually into two parts: a training part (70% of cases) and a test part (30% of cases). Verify the dimensions of the splits with function `dim()`.**

```{r}
set.seed(123) # for reproducibility
trainIndex <- createDataPartition(titanic$Survived, p = .7, times = 1, list = FALSE)

train <- titanic[trainIndex, ]
test <- titanic[-trainIndex, ]
```
We make use of the `createDataPartition()` function from package `caret` to generate the rownumbers for the splits. We could have also done this manually with e.g. `trainIndex <- sample(1:nrow(titanic), round(.7*nrow(titanic)), replace = TRUE)`. I find the `createDataPartition()` function always convenient, because it directly plugs into the `caret` functionality. 
```{r}
dim(train)
dim(test)
```
We can see that the split with `r dim(train)[1]` cases in the `train` set and `r dim(test)[1]` cases in the `test` set approximates the desired `p = .7` split probability with `r dim(train)[1]/nrow(titanic)`.

---

# Linear model

---

__Exercise 3__ **Predict `Age` from `Pclass`, `Sex` and `Survived`. Train your model on the `train` set and validate it on the `test` set**
```{r}
fit <- train %$% 
  lm(Age ~ Pclass + Sex + Survived) 
pred <- fit %>% 
  predict(newdata = test)
```
The `fit` object contains the model fitted on the training data. The `pred` object contains the predictions obtained by applying the `fit` model to the `test` data. 

---

__Exercise 4__ **Now calculate the RMSE and the $R^2$ for the predictions and compare those to the fitted model in `fit`**
```{r}
results <- data.frame(R2 = c(cor(pred, test$Age)^2, 
                             summary(fit)$r.squared),
                      RMSE = c((pred - test$Age)^2 %>% mean %>% sqrt, 
                               fit$residuals^2 %>% mean %>% sqrt))
rownames(results) <- c("predicted", "fitted")
```
We see that the $R^2$ is lower for the predictions and that the root mean squared error is higher. For unbiased estimators we can view the RMSE as the standard error of the estimator. The MSE would then be the variance of that unbiased estimator. 

---

__Exercise 5__ **Now use the `caret` package to do the same as above. Use the default paramters for the `train()` function and use the `train` data to train the model.**
```{r}
set.seed(123) # for reproducibility
# train the model on training set
model <- train(Age ~ Pclass + Sex + Survived,
               data = train,
               method = "lm")
model
```
We see that the `train` function by default uses a Bootstrapped resampling: the `train` data is resampled with replacement 25 times and every time the model is evaluated. Every individual sample is slightly different and, hence, the distribution of obtained results is also different. We can get information about the variance from:
```{r}
model$results
```

---

__Exercise 6__ **Now use the model from (5) to predict the `test` data and calculate the same metrics as in (4).**

```{r}
pred <- predict(model, newdata = test)
# R^2
cor(pred, test$Age)^2
# RMSE
(pred - test$Age)^2 %>% mean %>% sqrt
```
A much easier way of obtaining the same metrics is with the `postResample()` function:
```{r}
postResample(pred = pred, obs = test$Age)
```

---

__Exercise 7__ **Rerun the model from (5), but use 10-fold cross-validation on the training set. Evaluate the predictions with `postResample()`.**
```{r}
set.seed(123) # for reproducibility
model <- train(Age ~ Pclass + Sex + Survived,
               data = train,
               method = "lm",
               trControl = trainControl(method = "cv", number = 10)
               )
model
pred <- predict(model, newdata = test)
postResample(pred, test$Age)
```
There's not much more we can do for this linear model. At least we now that the below model is not grossly overfitted and that, if new data would come in, there is not much accuracy in predicting `Age` from these predictors. Let's hope that never happens. 
```{r}
lm(Age ~ Pclass + Sex + Survived, data = titanic) %>% summary()
```
We can still infer that `Age` differs over these groups. The overall model is highly significant. 

---

# Logistic regression

---

__Exercise 8__ **Use the same train/test splits to evaluate the performance of a logistic model where `Survived` is predicted from `Age`, `Pclass` and `Sex`. Study the accuracy and the confusion matrix**

We start by specifying the `caret` model. 
```{r}
set.seed(123) # for reproducibility
model <- train(Survived ~ Age + Pclass + Sex,
               data = train,
               method = "glm",
               family = binomial(link = "logit"),
               trControl = trainControl(method = "cv", number = 10)
               )
model
```
We can ask for a confusion matrix over the crossvalidated sets. 
```{r}
confusionMatrix(model)
```
We see that a bit over 80% is accurately predicted. The off-diagonal holds the other almost 20%. 

When we apply the model to the test data to obtain predictions, we can choose to get the `raw` predictions (i.e. the scale of the response as recorded in the data), or `prob` predictions (i.e. the scale of the response as modeled in probabilities). 
```{r}
pred <- predict(model, newdata = test, type = "raw")
```
The confusion matrix over the predictions yields many informative measures. 
```{r}
confusionMatrix(pred, test$Survived)
```

---

__Exercise 9__  **Compare the model obtained with `caret`'s `train()` on with a model obtained with `glm()`. Fit the `glm()` model on the training set. Study accuracy and parameters. **
We start with generating the relevant output from `glm()`. First, we fit the model with the correct family and link function
```{r}
fit <- train %$% 
  glm(Survived ~ Age + Pclass + Sex, 
      family = binomial(link = "logit"))
```
Next, we gemerate the predicted values:
```{r}
pred.glm <- ifelse(predict(fit, newdata = test, type = "response") > .5, "Yes", "No")
```
We have to indicate how to go from the predicted probabilities back to `No` and `Yes`. I use the `ifelse()` function to do that: if the probability is over .5, then the new value will be `Yes`, else it will be `No`. 

Now we can enter this vector of predicted `Yes` and `No` in the `postResample()` function to compare it with the observations in `test$Survived`. 
```{r}
postResample(pred.glm, test$Survived)
```
Finally, we can obtain the parameter summary from the fitted model. 
```{r}
fit %>% summary
```

When we obtain the same information from the `caret` model, we see that there are no differences. 
```{r}
postResample(pred, test$Survived)
model$finalModel %>% summary()
```
The outputs are identical. `caret` does not perform magical parameter poolings over the crossvalidated sets. The returned model is the fitted model. The accuracy that is obtained over the `train` set is obtained by meand of crossvalidation. The fitted model is in both cases identically applied to the `test` set. 

That said, the modeling possibilities with `caret` are enormous and there are many modeling efforts possible that lead to a proper model training on a training set. Crossvalidation is then needed. A test set can then be used to evaluate the performance of the trained model on unseen data. 

---

End of `Practical`. 