---
title: "How wrong may a useful model be?"
author: "Gerko Vink"
date: "Data Science and Predictive Machine Learning"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
---

## This lecture
- model fit

- model complexity

- peak into cross validation 

##  We use the following packages
```{r message=FALSE}
library(MASS)
library(dplyr)
library(magrittr)
library(ggplot2)
library(mice)
library(DAAG)
library(car)

set.seed(123)
```

# Model fit

## A simple model
```{r cache = FALSE}
boys.fit <- 
  na.omit(boys) %$% # Extremely wasteful
  lm(age ~ reg)
boys.fit

boys %>% na.omit(boys) %$% aggregate(age, list(reg), mean)
```

## Plotting the model
```{r fig.height=3.5}
means <- boys %>% na.omit(boys) %>% group_by(reg) %>% summarise(age = mean(age))
ggplot(na.omit(boys), aes(x = reg, y = age)) + 
  geom_point(color = "grey") + 
  geom_point(data = means, stat = "identity", size = 3)
```


## Model parameters
```{r cache = FALSE}
boys.fit %>%
  summary()
```

## Is it a good model? {.smaller}
```{r cache = FALSE}
boys.fit %>%
  anova()
```

It is not a very informative model. The `anova` is not significant, indicating that the contribution of the residuals is larger than the contribution of the model. 

The outcome `age` does not change significantly when `reg` is varied. 

## Model factors
```{r cache = FALSE}
boys.fit %>%
  model.matrix() %>%
  head(n = 10)
```

`R` expands the categorical variable for us

  - it dummy-codes the `5` categories into `4` dummies (and an intercept). 

## Post hoc comparisons
```{r cache = FALSE}
coef <- boys.fit %>% aov() %>% summary.lm()
coef
```

## Post hoc comparisons
Without adjustments for the p-value
```{r cache = FALSE}
na.omit(boys) %$% pairwise.t.test(age, reg, p.adj = "none")
```

## Post hoc comparisons
With adjusted p-values cf. Bonferoni correction
```{r cache = FALSE}
na.omit(boys) %$% pairwise.t.test(age, reg, p.adj = "bonf")
```

## Post hoc comparisons
Manually calculated
```{r}
p.val <- coef$coefficients
p.adjust(p.val[, "Pr(>|t|)"], method = "bonferroni")
```

If you have trouble reading scientific notation, `5.077098e-68` means the following

$$5.077098\text{e-68} = 5.077098 \times 10^{-68} = 5.077098 \times (\frac{1}{10})^{-68}$$

This indicates that the comma should be moved 68 places to the left:

$$5.077098\text{e-68} = .000000000000000000000000000000000000$$
$$000000000000000000000000000000005077098$$

## AIC
Akaike's *An Information Criterion* 
```{r cache = FALSE}
boys.fit %>% 
  AIC()
```

### What is AIC
AIC comes from information theory and can be used for model selection. The AIC quantifies the information that is lost by the statistical model, through the assumption that the data come from the same model. In other words: AIC measures the fit of the model to the data. 

- The better the fit, the less the loss in information
- AIC works on the log scale:
  - $\text{log}(0) = -\infty$, $\text{log}(1) = 0$, etc. 
- the closer the AIC is to $-\infty$, the better


# Model comparison

## A new model
Let's add predictor `hgt` to the model:
```{r cache = FALSE}
boys.fit2 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt)

boys.fit %>% AIC()
boys.fit2 %>% AIC()
```

## Another model
Let's add `wgt` to the model
```{r cache = FALSE}
boys.fit3 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt + wgt)
```

## And another model
Let's add `wgt` and the interaction between `wgt` and `hgt` to the model
```{r cache = FALSE}
boys.fit4 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt * wgt)
```
is equivalent to 
```{r eval=FALSE}
boys.fit4 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt + wgt + hgt:wgt)
```

## Model comparison
```{r cache = FALSE}
boys.fit %>% AIC()
boys.fit2 %>% AIC()
boys.fit3 %>% AIC()
boys.fit4 %>% AIC()
```

## Another form of model comparison
```{r cache = FALSE}
anova(boys.fit, boys.fit2, boys.fit3, boys.fit4)
```

## Inspect `boys.fit3`
```{r cache = FALSE}
boys.fit3 %>% anova()
```

## Inspect `boys.fit4`
```{r cache = FALSE}
boys.fit4 %>% anova()
```
It seems that `reg` and the interaction `hgt:wgt` are redundant

## Remove `reg`
```{r}
boys.fit5 <- 
  na.omit(boys) %$%
  lm(age ~ hgt + wgt)
```
Let's revisit the comparison
```{r}
anova(boys.fit, boys.fit2, boys.fit3, boys.fit5)
```
But the `boys.fit5` model is better than the previous model with fewer parameters

## Stepwise regression
We start with the full model, which contains all parameters for all columns. 

The most straightforward way to go about this is by specifying the following model:
```{r}
full.model <- lm(age ~ ., data = na.omit(boys))
full.model
```

## Stepwise regression - continued
We can then start with specifying the stepwise model. In this case we choose direction `both`. 
```{r}
step.model <- step(full.model, direction = "both", 
                      trace = FALSE)
step.model
```

Other options are 

- `forward`: fit all univariate models, add the best predictor and continue.
- `backward`: fit the full model, eliminate the worst predictor and continue. 

## Summary
```{r}
step.model %>% summary
```

## Stepwise regression - AIC
```{r}
full.model <- lm(age ~ ., data = na.omit(boys))
step.model <- MASS::stepAIC(full.model, direction = "both", 
                      trace = FALSE)
step.model
```

## Influence of cases
DfBeta calculates the change in coefficients depicted as deviation in SE's.
```{r cache = FALSE}
step.model %>%
  dfbeta() %>%
  head(n = 7)
```

# Prediction

## Fitted values
Let's use the simpler `anscombe` data example
```{r cache = FALSE}
fit <- anscombe %$% lm(y1 ~ x1)

y_hat <- 
  fit %>%
  fitted.values()
```
The residual is then calculated as
```{r cache = FALSE}
y_hat - anscombe$y1
```

## Predict new values
If we introduce new values for the predictor `x1`, we can generate predicted values from the model
```{r cache = FALSE, warning=FALSE}
new.x1 <- data.frame(x1 = 1:20)
fit %>% predict(newdata = new.x1)
```

## Predictions are draws from the regression line
```{r}
pred <- fit %>% predict(newdata = new.x1)
lm(pred ~ new.x1$x1)$coefficients
fit$coefficients
```

## Prediction intervals
```{r warning=FALSE}
fit %>% predict(interval = "prediction")
```

A prediction interval reflects the uncertainty around a single value. The confidence interval 
reflects the uncertainty around the mean prediction values. 

## How many cases are used?
```{r}
na.omit(boys) %$%
  lm(age ~ reg + hgt * wgt) %>%
  nobs()
```

If we would not have used `na.omit()`
```{r}
boys %$%
  lm(age ~ reg + hgt * wgt) %>%
  nobs()
```

# Confidence intervals?

## 95\% confidence interval {.smaller}
*If an infinite number of samples were drawn and CI's computed, then the true population mean $\mu$ would be in* ***at least*** *95\% of these intervals*

\[
95\%~CI=\bar{x}\pm{t}_{(1-\alpha/2)}\cdot SEM
\]

Example
```{r}
x.bar <- 7.6 # sample mean
SEM   <- 2.1 # standard error of the mean
n     <- 11 # sample size
df    <- n-1 # degrees of freedom
alpha <- .15 # significance level
t.crit <- qt(1 - alpha / 2, df) # t(1 - alpha / 2) for df = 10
c(x.bar - t.crit * SEM, x.bar + t.crit * SEM) 
```

## {.smaller}
<center>
<img src="Neyman1934.png" alt="HTML5 Icon" width = 75%>
</center>

        Neyman, J. (1934). On the Two Different Aspects of the Representative Method: 
        The Method of Stratified Sampling and the Method of Purposive Selection. 
        Journal of the Royal Statistical Society, Vol. 97, No. 4 (1934), pp. 558-625

## Misconceptions {.smaller}
Confidence intervals are frequently misunderstood, even well-established researchers sometimes misinterpret them. .

1. A realised 95% CI does not mean:

- that there is a 95% probability the population parameter lies within the interval
- that there is a 95% probability that the interval covers the population parameter

    Once an experiment is done and an interval is calculated, the interval either covers, or does       not cover the parameter value. Probability is no longer involved. 

    The 95% probability only has to do with the estimation procedure. 

2. A 95% confidence interval does not mean that 95% of the sample data lie within the interval.
3. A confidence interval is not a range of plausible values for the sample mean, though it may be understood as an estimate of plausible values for the population parameter.
4. A particular confidence interval of 95% calculated from an experiment does not mean that there is a 95% probability of a sample mean from a repeat of the experiment falling within this interval.

## Confidence intervals
```{r fig.height = 4, echo=FALSE, message=FALSE, message=FALSE, warning = FALSE}
set.seed(1234)
library(plyr)
samples <- rlply(100, rnorm(5000, mean = 0, sd = 1))
info <- function(x){ 
  M <- mean(x)
  DF <- length(x) - 1
  SE <- 1 / sqrt(length(x))
  INT <- qt(.975, DF) * SE
  return(c(M, M - 0, SE, M - INT, M + INT))
}
format <- c("Mean" = 0, "Bias" = 0, "Std.Err" = 0, "Lower" = 0, "Upper" = 0)
require("magrittr")
results <- samples %>%
  vapply(., info, format) %>%
  t()
results <- results %>%
  as.data.frame() %>%
  mutate(Covered = Lower < 0 & 0 < Upper)
require(ggplot2)
limits <- aes(ymax = results$Upper, ymin = results$Lower)
ggplot(results, aes(y=Mean, x=1:100, colour = Covered)) + 
  geom_hline(aes(yintercept = 0), color = "dark grey", size = 2) + 
  geom_pointrange(limits) + 
  xlab("Simulations 1-100") +
  ylab("Means and 95% Confidence Intervals")
```

100 simulated samples from a population with $\mu = 0$ and $\sigma^2=1$. Out of 100 samples, only 5 samples have confidence intervals that do not cover the population mean.

## For fun
<center> 
![](img/going_out_for_a_byte.png){width=40%}
<br>[source](https://www.reddit.com/r/ProgrammerHumor/comments/8za9b6/hello_i_am_your_server_for_today/)
</center>
